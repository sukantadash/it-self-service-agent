{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb24293-6e3b-4cee-b9b2-82a29d344b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-stack-client==0.3.0\n",
      "  Downloading llama_stack_client-0.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (4.12.1)\n",
      "Collecting click (from llama-stack-client==0.3.0)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from llama-stack-client==0.3.0)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fire (from llama-stack-client==0.3.0)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (0.28.1)\n",
      "Collecting pandas (from llama-stack-client==0.3.0)\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (3.0.52)\n",
      "Collecting pyaml (from llama-stack-client==0.3.0)\n",
      "  Downloading pyaml-26.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from llama-stack-client==0.3.0)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (2.32.5)\n",
      "Collecting rich (from llama-stack-client==0.3.0)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sniffio (from llama-stack-client==0.3.0)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting termcolor (from llama-stack-client==0.3.0)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting tqdm (from llama-stack-client==0.3.0)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.12/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.3.0) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.3.0) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas->llama-stack-client==0.3.0)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.12/site-packages (from prompt-toolkit->llama-stack-client==0.3.0) (0.2.14)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.12/site-packages (from pyaml->llama-stack-client==0.3.0) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/app-root/lib64/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (2.6.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->llama-stack-client==0.3.0)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.12/site-packages (from rich->llama-stack-client==0.3.0) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->llama-stack-client==0.3.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.3.0) (1.17.0)\n",
      "Downloading llama_stack_client-0.3.0-py3-none-any.whl (425 kB)\n",
      "Downloading pypdf-6.7.0-py3-none-any.whl (330 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m162.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m226.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-26.2.1-py3-none-any.whl (27 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m219.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, termcolor, sniffio, pypdf, pydantic-core, pyaml, numpy, mdurl, distro, click, annotated-types, pydantic, pandas, markdown-it-py, fire, rich, llama-stack-client\n",
      "Successfully installed annotated-types-0.7.0 click-8.3.1 distro-1.9.0 fire-0.7.1 llama-stack-client-0.3.0 markdown-it-py-4.0.0 mdurl-0.1.2 numpy-2.4.2 pandas-3.0.0 pyaml-26.2.1 pydantic-2.12.5 pydantic-core-2.41.5 pypdf-6.7.0 rich-14.3.2 sniffio-1.3.1 termcolor-3.3.0 tqdm-4.67.3 typing-inspection-0.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U llama-stack-client==0.3.0 pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7218aa51-042d-4380-9133-6197a07757f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_7a490b87-d4f6-479d-9733-a28d135bc783\n",
      "status: completed\n",
      "model: llama-17b/llama-4-scout-17b-16e-w4a16\n",
      "--------------------------------------------------------------------------------\n",
      "output_text:\n",
      "CANDIDATES:\n",
      "1) app_id=oom-test-app | app_name=out memory app | namespace=oom-test | source_cluster=console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com | destination_cluster=console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\n",
      "--------------------------------------------------------------------------------\n",
      "output_items: 3\n",
      "[0] type=mcp_list_tools\n",
      "[1] type=mcp_call\n",
      "[2] type=message\n",
      "--------------------------------------------------------------------------------\n",
      "mcp_call: server_label= app-migration-utility name= search_app_catalog error= None output_preview= [{\"namespace\":\"oom-test\",\"app_name\":\"out memory app\",\"source_cluster\":\"console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com\",\"destination_cluster\":\"console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\",\"app_id\":\"oom-test-app\"}]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMASTACK_BASE_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
    "MODEL = \"llama-17b/llama-4-scout-17b-16e-w4a16\"\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful type-a app migration assistant. Speak directly to users in a \"\n",
    "    \"conversational and professional manner. CRITICAL do not share your internal thinking\"\n",
    ")\n",
    "\n",
    "# From pod logs: current_state_name=search_app_catalog_normalize\n",
    "USER_PROMPT = \"\"\"You are an app migration assistant.\n",
    "\n",
    "CRITICAL: Execute tool calls silently. Do not announce them.\n",
    "\n",
    "Search hint (may be empty): \"oom\"\n",
    "Latest user message: \"oom-test-app\"\n",
    "\n",
    "Call search_app_catalog with:\n",
    "- q = the Search hint if it is non-empty; otherwise use the Latest user message\n",
    "- limit=50\n",
    "- offset=0\n",
    "\n",
    "After the tool returns, output a normalized candidate list (unique tuples only).\n",
    "- If none, output exactly: CANDIDATES: NONE\n",
    "- Otherwise output:\n",
    "  CANDIDATES:\n",
    "  1) app_id=<...> | app_name=<...> | namespace=<...> | source_cluster=<...> | destination_cluster=<...>\n",
    "  2) ...\n",
    "Max 50 candidates. Do not invent values.\n",
    "\"\"\"\n",
    "\n",
    "# From logs/config:\n",
    "# - tools include KB file_search with vector_store_id vs_66507187-b021-4b8b-82ca-a2f277d3fcef\n",
    "# - and MCP servers openshift/app-migration-utility/snow\n",
    "# - state allowed_tools=[\"search_app_catalog\"] so MCP tools are restricted to that tool name\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"app-migration-utility\",\n",
    "        \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\",\n",
    "    }\n",
    "]\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMASTACK_BASE_URL)\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    "    temperature=TEMPERATURE,\n",
    "    tools=TOOLS,\n",
    ")\n",
    "\n",
    "print(\"response_id:\", getattr(resp, \"id\", None))\n",
    "print(\"status:\", getattr(resp, \"status\", None))\n",
    "print(\"model:\", getattr(resp, \"model\", None))\n",
    "print(\"-\" * 80)\n",
    "print(\"output_text:\")\n",
    "print(getattr(resp, \"output_text\", \"\") or \"\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extra debug: print output item types + any mcp_call summary\n",
    "out = getattr(resp, \"output\", None) or []\n",
    "print(\"output_items:\", len(out))\n",
    "for i, item in enumerate(out):\n",
    "    print(f\"[{i}] type={getattr(item, 'type', None)}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "for item in out:\n",
    "    if str(getattr(item, \"type\", \"\")) == \"mcp_call\":\n",
    "        print(\n",
    "            \"mcp_call:\",\n",
    "            \"server_label=\", getattr(item, \"server_label\", None),\n",
    "            \"name=\", getattr(item, \"name\", None),\n",
    "            \"error=\", getattr(item, \"error\", None),\n",
    "            \"output_preview=\", (getattr(item, \"output\", None) or \"\")[:300],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e742f866-8e66-4ca2-927c-c2c2962a8050",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1074390221.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mExtract the fields from the below pod logs and write the below python code. if there are multiple requests response then write multiple calls to llama-stack with those requests\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#prompt to gemini model\n",
    "\n",
    "\n",
    "I am debugging the pod logs. Extract the exact and complete AGENT_NAME,STATE_NAME,TEMPERATURE, system message, user message, tools from the shared pod logs donot change them and write the below python code. if there are multiple requests response then write multiple calls to llama-stack with those requests\n",
    "\n",
    "---------------\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMASTACK_BASE_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
    "MODEL = \"llama-17b/llama-4-scout-17b-16e-w4a16\"\n",
    "\n",
    "AGENT_NAME =\n",
    "STATE_NAME = \n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "SYSTEM_MESSAGE = (\n",
    ")\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "TOOLS = []\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMASTACK_BASE_URL)\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    "    temperature=TEMPERATURE,\n",
    "    tools=TOOLS,\n",
    ")\n",
    "\n",
    "print(\"response_id:\", getattr(resp, \"id\", None))\n",
    "print(\"status:\", getattr(resp, \"status\", None))\n",
    "print(\"model:\", getattr(resp, \"model\", None))\n",
    "print(\"-\" * 80)\n",
    "print(\"output_text:\")\n",
    "print(getattr(resp, \"output_text\", \"\") or \"\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extra debug: print output item types + any mcp_call summary\n",
    "out = getattr(resp, \"output\", None) or []\n",
    "print(\"output_items:\", len(out))\n",
    "for i, item in enumerate(out):\n",
    "    print(f\"[{i}] type={getattr(item, 'type', None)}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "for item in out:\n",
    "    if str(getattr(item, \"type\", \"\")) == \"mcp_call\":\n",
    "        print(\n",
    "            \"mcp_call:\",\n",
    "            \"server_label=\", getattr(item, \"server_label\", None),\n",
    "            \"name=\", getattr(item, \"name\", None),\n",
    "            \"error=\", getattr(item, \"error\", None),\n",
    "            \"output_preview=\", (getattr(item, \"output\", None) or \"\")[:300],\n",
    "        )\n",
    "------------\n",
    "\n",
    "pod logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adfe75d4-ff9b-452d-8d69-054b10870c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMASTACK_BASE_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
    "MODEL = \"llama-17b/llama-4-scout-17b-16e-w4a16\"\n",
    "client = LlamaStackClient(base_url=LLAMASTACK_BASE_URL)\n",
    "\n",
    "def call_llama_stack(agent_name, state_name, temperature, system_msg, user_prompt, tools):\n",
    "    print(f\"--- Calling Agent: {agent_name} | State: {state_name} ---\")\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        tools=tools,\n",
    "    )\n",
    "    \n",
    "    print(\"response_id:\", getattr(resp, \"id\", None))\n",
    "    print(\"status:\", getattr(resp, \"status\", None))\n",
    "    print(\"output_text:\")\n",
    "    print(getattr(resp, \"output_text\", \"\") or \"\")\n",
    "    \n",
    "    out = getattr(resp, \"output\", None) or []\n",
    "    for i, item in enumerate(out):\n",
    "        if str(getattr(item, \"type\", \"\")) == \"mcp_call\":\n",
    "            print(f\"mcp_call [{i}]: {getattr(item, 'name', None)}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafd6eca-54da-4ebd-bab1-be96d9e85267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calling Agent: routing-agent | State: classify_user_intent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_1a691b6e-c531-4baf-96ff-034fcb606f04\n",
      "status: completed\n",
      "output_text:\n",
      "Based on the user's request, \"I want to migrate an application\", I would categorize their request as:\n",
      "\n",
      "TYPE_A_MIGRATION\n",
      "\n",
      "The user explicitly mentions migrating an application, which aligns with the examples provided for TYPE_A_MIGRATION.\n",
      "--------------------------------------------------------------------------------\n",
      "--- Calling Agent: type-a-app-migration | State: derive_app_search_q ---\n",
      "response_id: resp_d4f0f6e3-5676-49cd-9ede-64f05bb28461\n",
      "status: completed\n",
      "output_text:\n",
      "Q: NONE\n",
      "--------------------------------------------------------------------------------\n",
      "--- Calling Agent: type-a-app-migration | State: classify_app_query_intent ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_fad34e02-1ca3-447d-98a2-94458bfd8e16\n",
      "status: completed\n",
      "output_text:\n",
      "QUERY\n",
      "--------------------------------------------------------------------------------\n",
      "--- Calling Agent: type-a-app-migration | State: search_app_catalog_normalize ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_bb79bfad-4d0b-40f2-ad0a-375b4a255aa4\n",
      "status: completed\n",
      "output_text:\n",
      "CANDIDATES:\n",
      "1) app_id=oom-test-app | app_name=out memory app | namespace=oom-test | source_cluster=console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com | destination_cluster=console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\n",
      "mcp_call [2]: search_app_catalog\n",
      "--------------------------------------------------------------------------------\n",
      "--- Calling Agent: type-a-app-migration | State: classify_discovery_confirmation ---\n",
      "response_id: resp_4d7e00f9-b65e-410d-87a9-7f719459b74a\n",
      "status: completed\n",
      "output_text:\n",
      "YES\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- REQUEST 1: routing-agent (classify_user_intent) ---\n",
    "call_llama_stack(\n",
    "    agent_name=\"routing-agent\",\n",
    "    state_name=\"classify_user_intent\",\n",
    "    temperature=0.1,\n",
    "    system_msg=\"You are a routing agent specializing in getting users to the correct specialist agent. Be helpful, friendly, and efficient in determining their needs.\",\n",
    "    user_prompt=\"\"\"The user said: \"I want to migrate an application\"\\n\\nAnalyze their request and determine what they need help with:\\n\\n1. LAPTOP_REFRESH - User wants help with laptop refresh, replacement, new laptop, laptop upgrade, hardware refresh. Also includes requests that just say \"refresh\" or similar terms.\\n2. EMAIL_CHANGE - User wants to update, change, or modify their email address\\n3. TYPE_A_MIGRATION - User wants help migrating a virtual application (e.g., OVA/VMware) to Red Hat OpenShift Virtualization, or mentions MTV (Migration Toolkit for Virtualization), OpenShift Virtualization, KubeVirt, NFS-hosted OVA migration plan, or similar migration workflow requests.\\n4. OTHER - User needs help with something else or request is unclear\\n\\nExamples of LAPTOP_REFRESH requests:\\n- \"I need a new laptop\"\\n- \"laptop refresh\"\\n- \"refresh\"\\n- \"I want to refresh my laptop\"\\n- \"hardware refresh\"\\n\\nExamples of TYPE_A_MIGRATION requests:\\n- \"I need to migrate my app to OpenShift Virtualization\"\\n- \"app migration\"\\n- \"migration\"\\n- \"I want to migrate my app\"\\n- \"application migration\"\\n\\nRespond with exactly one of: LAPTOP_REFRESH, EMAIL_CHANGE, TYPE_A_MIGRATION, or OTHER\\n\"\"\",\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# --- REQUEST 2: type-a-app-migration (derive_app_search_q) ---\n",
    "call_llama_stack(\n",
    "    agent_name=\"type-a-app-migration\",\n",
    "    state_name=\"derive_app_search_q\",\n",
    "    temperature=0.1,\n",
    "    system_msg=\"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "    user_prompt=\"\"\"You are a migration assistant.\\nExtract the specific application name, ID, or namespace from the user message.\\n\\nCRITICAL: If the user message is generic (e.g., \"migration\", \"ooo\", \"help me\"),\\noutput EXACTLY:\\nQ: NONE\\n\\nUser message: \"I want to migrate an application\"\\n\\nOutput ONLY:\\nQ: <search_substring>\\n\"\"\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]}, {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}, {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}]\n",
    ")\n",
    "\n",
    "# --- REQUEST 3: type-a-app-migration (classify_app_query_intent) ---\n",
    "call_llama_stack(\n",
    "    agent_name=\"type-a-app-migration\",\n",
    "    state_name=\"classify_app_query_intent\",\n",
    "    temperature=0.2,\n",
    "    system_msg=\"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "    user_prompt=\"\"\"The user said: \"oom-test-app\"\\n\\nDetermine intent:\\n- RETURN_TO_ROUTER: user wants to go back/stop/cancel\\n- SWITCH_TASK: user is switching to a different IT task/topic (e.g., \"laptop refresh\", \"email change\")\\n- QUERY: user is providing a search term or application identifier to look up\\n- UNCLEAR: doesn't contain a usable query\\n\\nRespond with only: RETURN_TO_ROUTER, SWITCH_TASK, QUERY, or UNCLEAR\\n\"\"\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]}, {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}, {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}]\n",
    ")\n",
    "\n",
    "# --- REQUEST 4: type-a-app-migration (search_app_catalog_normalize) ---\n",
    "call_llama_stack(\n",
    "    agent_name=\"type-a-app-migration\",\n",
    "    state_name=\"search_app_catalog_normalize\",\n",
    "    temperature=0.2,\n",
    "    system_msg=\"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "    user_prompt=\"\"\"You are an app migration assistant.\\n\\nCRITICAL: Execute tool calls silently. Do not announce them.\\n\\nSearch hint (may be empty): \\nLatest user message: \"oom-test-app\"\\n\\nCall search_app_catalog with:\\n- q = the Search hint if it is non-empty; otherwise use the Latest user message\\n- limit=50\\n- offset=0\\n\\nAfter the tool returns, output a normalized candidate list (unique tuples only).\\n- If none, output exactly: CANDIDATES: NONE\\n- Otherwise output:\\n  CANDIDATES:\\n  1) app_id=<...> | app_name=<...> | namespace=<...> | source_cluster=<...> | destination_cluster=<...>\\n  2) ...\\nMax 50 candidates. Do not invent values.\\n\"\"\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]}, {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]}, {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]}]\n",
    ")\n",
    "\n",
    "# --- REQUEST 5: type-a-app-migration (classify_discovery_confirmation) ---\n",
    "call_llama_stack(\n",
    "    agent_name=\"type-a-app-migration\",\n",
    "    state_name=\"classify_discovery_confirmation\",\n",
    "    temperature=0.2,\n",
    "    system_msg=\"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "    user_prompt=\"\"\"The user was asked: \"Should I proceed with the resource discovery for this application?\"\\nThey responded: \"yes\"\\n\\nClassify their intent:\\n- YES: user wants to proceed with discovery\\n- NO: user does not want to proceed\\n- RETURN_TO_ROUTER: user wants to cancel/stop/return to router\\n- UNCLEAR: ambiguous\\n\\nRespond with only: YES, NO, RETURN_TO_ROUTER, or UNCLEAR\\n\"\"\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]}, {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}, {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a66d99b1-88a6-4db8-b649-b55e08c5e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Call 1: type-a-app-migration - search_app_catalog_normalize\n",
      "USER_PROMPT: You are an app migration assistant.\n",
      "\n",
      "CRITICAL: Execute tool calls silently. Do not announce them.\n",
      "\n",
      "Search hint (may be empty): \n",
      "Latest user message: \"oom-test-app\"\n",
      "\n",
      "Call search_app_catalog with:\n",
      "- q = the Search hint if it is non-empty; otherwise use the Latest user message\n",
      "- limit=50\n",
      "- offset=0\n",
      "\n",
      "After the tool returns, output a normalized candidate list (unique tuples only).\n",
      "- If none, output exactly: CANDIDATES: NONE\n",
      "- Otherwise output:\n",
      "  CANDIDATES:\n",
      "  1) app_id=<...> | app_name=<...> | namespace=<...> | source_cluster=<...> | destination_cluster=<...>\n",
      "  2) ...\n",
      "Max 50 candidates. Do not invent values.\n",
      "\n",
      "TOOLS: [{'type': 'file_search', 'vector_store_ids': ['vs_66507187-b021-4b8b-82ca-a2f277d3fcef']}, {'type': 'mcp', 'server_label': 'openshift', 'server_url': 'http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp', 'require_approval': 'never', 'headers': {'AUTHORITATIVE_USER_ID': 'sudash@redhat.com', 'SERVICE_NOW_TOKEN': 'now_mock_api_key'}, 'allowed_tools': ['search_app_catalog']}, {'type': 'mcp', 'server_label': 'app-migration-utility', 'server_url': 'http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp', 'require_approval': 'never', 'headers': {'AUTHORITATIVE_USER_ID': 'sudash@redhat.com', 'SERVICE_NOW_TOKEN': 'now_mock_api_key'}, 'allowed_tools': ['search_app_catalog']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_6ce76fa2-fc0c-45c2-94d0-6b7940ee9b14\n",
      "status: completed\n",
      "model: llama-17b/llama-4-scout-17b-16e-w4a16\n",
      "--------------------------------------------------------------------------------\n",
      "output_text:\n",
      "CANDIDATES:\n",
      "1) app_id=oom-test-app | app_name=out memory app | namespace=oom-test | source_cluster=console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com | destination_cluster=console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\n",
      "--------------------------------------------------------------------------------\n",
      "output_items: 4\n",
      "[0] type=mcp_list_tools\n",
      "[1] type=mcp_list_tools\n",
      "[2] type=mcp_call\n",
      "[3] type=message\n",
      "--------------------------------------------------------------------------------\n",
      "mcp_call: server_label= app-migration-utility name= search_app_catalog error= None output_preview= [{\"namespace\":\"oom-test\",\"app_name\":\"out memory app\",\"source_cluster\":\"console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com\",\"destination_cluster\":\"console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\",\"app_id\":\"oom-test-app\"}]\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Executing Call 2: type-a-app-migration - format_lookup_status (FAILED STATE)\n",
      "USER_PROMPT: You are an app migration assistant.\n",
      "\n",
      "CANDIDATES:\n",
      "1) app_id=oom-test-app | app_name=out memory app | namespace=oom-test | source_cluster=console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com | destination_cluster=console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\n",
      "\n",
      "Your job is to present the catalog lookup result to the user in a conversational way.\n",
      "DO NOT include internal markers like \"STATUS:\" in your response.\n",
      "\n",
      "If 0 matches, respond exactly:\n",
      "I couldn’t find any matching apps in the catalog. Please provide the Application Name, App ID, or Namespace.\n",
      "\n",
      "If exactly 1 match, respond exactly in this format:\n",
      "I've located <APP_NAME> (ID: <APP_ID>) in namespace <NAMESPACE>. Should I proceed with the resource discovery for this application?\n",
      "\n",
      "If multiple matches, respond exactly in this format:\n",
      "I found multiple matches for that application. Which one should we proceed with?\n",
      "\n",
      "1) ID: <APP_ID> | Name: <APP_NAME> | Namespace: <NAMESPACE> | Cluster: <SOURCE_CLUSTER>\n",
      "2) ID: ...\n",
      "\n",
      "Please reply with the number (1 to N) or the App ID.\n",
      "\n",
      "Rules:\n",
      "- Use unique tuples only\n",
      "- Show up to 15 options\n",
      "- Use SOURCE_CLUSTER value for \"Cluster\"\n",
      "- Do not invent values\n",
      "\n",
      "TOOLS: [{'type': 'file_search', 'vector_store_ids': ['vs_66507187-b021-4b8b-82ca-a2f277d3fcef']}, {'type': 'mcp', 'server_label': 'openshift', 'server_url': 'http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp', 'require_approval': 'never', 'headers': {'AUTHORITATIVE_USER_ID': 'sudash@redhat.com', 'SERVICE_NOW_TOKEN': 'now_mock_api_key'}, 'allowed_tools': ['']}, {'type': 'mcp', 'server_label': 'app-migration-utility', 'server_url': 'http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp', 'require_approval': 'never', 'headers': {'AUTHORITATIVE_USER_ID': 'sudash@redhat.com', 'SERVICE_NOW_TOKEN': 'now_mock_api_key'}, 'allowed_tools': ['']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_id: resp_7bff7014-5831-4862-b1bf-299fd54ffef6\n",
      "status: completed\n",
      "model: llama-17b/llama-4-scout-17b-16e-w4a16\n",
      "--------------------------------------------------------------------------------\n",
      "output_text:\n",
      "I've located out memory app (ID: oom-test-app) in namespace oom-test. Should I proceed with the resource discovery for this application?\n",
      "--------------------------------------------------------------------------------\n",
      "output_items: 3\n",
      "[0] type=mcp_list_tools\n",
      "[1] type=mcp_list_tools\n",
      "[2] type=message\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "LLAMASTACK_BASE_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
    "MODEL = \"llama-17b/llama-4-scout-17b-16e-w4a16\"\n",
    "\n",
    "requests = [\n",
    "    {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"search_app_catalog_normalize\",\n",
    "        \"TEMPERATURE\": 0.2,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"You are an app migration assistant.\n",
    "\n",
    "CRITICAL: Execute tool calls silently. Do not announce them.\n",
    "\n",
    "Search hint (may be empty): \n",
    "Latest user message: \"oom-test-app\"\n",
    "\n",
    "Call search_app_catalog with:\n",
    "- q = the Search hint if it is non-empty; otherwise use the Latest user message\n",
    "- limit=50\n",
    "- offset=0\n",
    "\n",
    "After the tool returns, output a normalized candidate list (unique tuples only).\n",
    "- If none, output exactly: CANDIDATES: NONE\n",
    "- Otherwise output:\n",
    "  CANDIDATES:\n",
    "  1) app_id=<...> | app_name=<...> | namespace=<...> | source_cluster=<...> | destination_cluster=<...>\n",
    "  2) ...\n",
    "Max 50 candidates. Do not invent values.\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]}\n",
    "        ]\n",
    "    }, {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"format_lookup_status (FAILED STATE)\",\n",
    "        \"TEMPERATURE\": 0.1,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"You are an app migration assistant.\n",
    "\n",
    "CANDIDATES:\n",
    "1) app_id=oom-test-app | app_name=out memory app | namespace=oom-test | source_cluster=console-openshift-console.apps.cluster-5n8tc.5n8tc.sandbox3547.opentlc.com | destination_cluster=console-openshift-console.apps.cluster-xyz.xyz.sandboxxyz.opentlc.com\n",
    "\n",
    "Your job is to present the catalog lookup result to the user in a conversational way.\n",
    "DO NOT include internal markers like \"STATUS:\" in your response.\n",
    "\n",
    "If 0 matches, respond exactly:\n",
    "I couldn’t find any matching apps in the catalog. Please provide the Application Name, App ID, or Namespace.\n",
    "\n",
    "If exactly 1 match, respond exactly in this format:\n",
    "I've located <APP_NAME> (ID: <APP_ID>) in namespace <NAMESPACE>. Should I proceed with the resource discovery for this application?\n",
    "\n",
    "If multiple matches, respond exactly in this format:\n",
    "I found multiple matches for that application. Which one should we proceed with?\n",
    "\n",
    "1) ID: <APP_ID> | Name: <APP_NAME> | Namespace: <NAMESPACE> | Cluster: <SOURCE_CLUSTER>\n",
    "2) ID: ...\n",
    "\n",
    "Please reply with the number (1 to N) or the App ID.\n",
    "\n",
    "Rules:\n",
    "- Use unique tuples only\n",
    "- Show up to 15 options\n",
    "- Use SOURCE_CLUSTER value for \"Cluster\"\n",
    "- Do not invent values\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "client = LlamaStackClient(base_url=LLAMASTACK_BASE_URL)\n",
    "\n",
    "for i, req in enumerate(requests):\n",
    "    print(f\"Executing Call {i+1}: {req['AGENT_NAME']} - {req['STATE_NAME']}\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        print(\"USER_PROMPT:\", req[\"USER_PROMPT\"])\n",
    "        print(\"TOOLS:\", req[\"TOOLS\"])\n",
    "        \n",
    "        resp = client.responses.create(\n",
    "            model=MODEL,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": req[\"SYSTEM_MESSAGE\"]},\n",
    "                {\"role\": \"user\", \"content\": req[\"USER_PROMPT\"]},\n",
    "            ],\n",
    "            temperature=req[\"TEMPERATURE\"],\n",
    "            tools=req[\"TOOLS\"],\n",
    "        )\n",
    "\n",
    "        print(\"response_id:\", getattr(resp, \"id\", None))\n",
    "        print(\"status:\", getattr(resp, \"status\", None))\n",
    "        print(\"model:\", getattr(resp, \"model\", None))\n",
    "        print(\"-\" * 80)\n",
    "        print(\"output_text:\")\n",
    "        print(getattr(resp, \"output_text\", \"\") or \"\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Extra debug: print output item types + any mcp_call summary\n",
    "        out = getattr(resp, \"output\", None) or []\n",
    "        print(\"output_items:\", len(out))\n",
    "        for j, item in enumerate(out):\n",
    "            print(f\"[{j}] type={getattr(item, 'type', None)}\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        for item in out:\n",
    "            if str(getattr(item, \"type\", \"\")) == \"mcp_call\":\n",
    "                print(\n",
    "                    \"mcp_call:\",\n",
    "                    \"server_label=\", getattr(item, \"server_label\", None),\n",
    "                    \"name=\", getattr(item, \"name\", None),\n",
    "                    \"error=\", getattr(item, \"error\", None),\n",
    "                    \"output_preview=\", (getattr(item, \"output\", None) or \"\")[:300],\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a747c76-f091-457b-956c-2d700786ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted requests from pod logs\n",
    "requests = [\n",
    "    {\n",
    "        \"AGENT_NAME\": \"routing-agent\",\n",
    "        \"STATE_NAME\": \"greet_and_identify_need\",\n",
    "        \"TEMPERATURE\": 0.3,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a routing agent specializing in getting users to the correct specialist agent. Be helpful, friendly, and efficient in determining their needs.\",\n",
    "        \"USER_PROMPT\": \"\"\"You are a routing agent specializing in getting users to the correct specialist agent. Be helpful, friendly, and efficient in determining their needs.\n",
    "\n",
    "Greet the user and clearly identify yourself as the routing agent. Ask them what they need help with today.\n",
    "\n",
    "Currently you can help with:\n",
    "- Laptop refresh requests\n",
    "- Email address updates\n",
    "- Type-A application migration (OVA -> OpenShift Virtualization using MTV)\n",
    "\n",
    "Ask them to describe what they need help with.\n",
    "\"\"\",\n",
    "        \"TOOLS\": []\n",
    "    },\n",
    "    {\n",
    "        \"AGENT_NAME\": \"routing-agent\",\n",
    "        \"STATE_NAME\": \"classify_user_intent\",\n",
    "        \"TEMPERATURE\": 0.1,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a routing agent specializing in getting users to the correct specialist agent. Be helpful, friendly, and efficient in determining their needs.\",\n",
    "        \"USER_PROMPT\": \"\"\"The user said: \"I want to migrate an application\"\n",
    "\n",
    "Analyze their request and determine what they need help with:\n",
    "\n",
    "1. LAPTOP_REFRESH - User wants help with laptop refresh, replacement, new laptop, laptop upgrade, hardware refresh. Also includes requests that just say \"refresh\" or similar terms.\n",
    "2. EMAIL_CHANGE - User wants to update, change, or modify their email address\n",
    "3. TYPE_A_MIGRATION - User wants help migrating a virtual application (e.g., OVA/VMware) to Red Hat OpenShift Virtualization, or mentions MTV (Migration Toolkit for Virtualization), OpenShift Virtualization, KubeVirt, NFS-hosted OVA migration plan, or similar migration workflow requests.\n",
    "4. OTHER - User needs help with something else or request is unclear\n",
    "\n",
    "Examples of LAPTOP_REFRESH requests:\n",
    "- \"I need a new laptop\"\n",
    "- \"laptop refresh\"\n",
    "- \"refresh\"\n",
    "- \"I want to refresh my laptop\"\n",
    "- \"hardware refresh\"\n",
    "\n",
    "Examples of TYPE_A_MIGRATION requests:\n",
    "- \"I need to migrate my app to OpenShift Virtualization\"\n",
    "- \"app migration\"\n",
    "- \"migration\"\n",
    "- \"I want to migrate my app\"\n",
    "- \"application migration\"\n",
    "\n",
    "Respond with exactly one of: LAPTOP_REFRESH, EMAIL_CHANGE, TYPE_A_MIGRATION, or OTHER\n",
    "\"\"\",\n",
    "        \"TOOLS\": []\n",
    "    },\n",
    "    {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"derive_app_search_q\",\n",
    "        \"TEMPERATURE\": 0.1,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"You are a migration assistant.\n",
    "Extract the specific application name, ID, or namespace from the user message.\n",
    "\n",
    "CRITICAL: If the user message is generic (e.g., \"migration\", \"ooo\", \"help me\"),\n",
    "output EXACTLY:\n",
    "Q: NONE\n",
    "\n",
    "User message: \"I want to migrate an application\"\n",
    "\n",
    "Output ONLY:\n",
    "Q: <search_substring>\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"classify_app_query_intent\",\n",
    "        \"TEMPERATURE\": 0.2,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"The user said: \"oom-test-app\"\n",
    "\n",
    "Determine intent:\n",
    "- RETURN_TO_ROUTER: user wants to go back/stop/cancel\n",
    "- SWITCH_TASK: user is switching to a different IT task/topic (e.g., \"laptop refresh\", \"email change\")\n",
    "- QUERY: user is providing a search term or application identifier to look up\n",
    "- UNCLEAR: doesn't contain a usable query\n",
    "\n",
    "Respond with only: RETURN_TO_ROUTER, SWITCH_TASK, QUERY, or UNCLEAR\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"search_app_catalog_normalize\",\n",
    "        \"TEMPERATURE\": 0.2,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"You are an app migration assistant.\n",
    "\n",
    "CRITICAL: DO NOT announce what you are about to do. Execute tool calls silently.\n",
    "\n",
    "Execute:\n",
    "search_app_catalog(q=\"oom-test-app\", limit=50, offset=0)\n",
    "\n",
    "After the tool returns, output a normalized candidate list (unique tuples only).\n",
    "- If none found, output exactly: CANDIDATES: NONE\n",
    "- Otherwise output:\n",
    "  CANDIDATES:\n",
    "  1) app_id=<...> | app_name=<...> | namespace=<...> | source_cluster=<...> | destination_cluster=<...>\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"search_app_catalog\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"AGENT_NAME\": \"type-a-app-migration\",\n",
    "        \"STATE_NAME\": \"format_lookup_status (FAILED STATE)\",\n",
    "        \"TEMPERATURE\": 0.1,\n",
    "        \"SYSTEM_MESSAGE\": \"You are a helpful type-a app migration assistant. Speak directly to users in a conversational and professional manner. CRITICAL do not share your internal thinking\",\n",
    "        \"USER_PROMPT\": \"\"\"You are an app migration assistant.\n",
    "\n",
    "Candidates:\n",
    "search_app_catalog(q=\"oom-test-app\", limit=50, offset=0)\n",
    "\n",
    "Your job is to present the catalog lookup result to the user in a conversational way.\n",
    "DO NOT include internal markers like \"STATUS:\" in your response.\n",
    "\n",
    "If 0 matches, respond exactly:\n",
    "I couldn’t find any matching apps in the catalog. Please provide the Application Name, App ID, or Namespace.\n",
    "\n",
    "If exactly 1 match, respond exactly in this format:\n",
    "I've located <APP_NAME> (ID: <APP_ID>) in namespace <NAMESPACE>. Should I proceed with the resource discovery for this application?\n",
    "\n",
    "If multiple matches, respond exactly in this format:\n",
    "I found multiple matches for that application. Which one should we proceed with?\n",
    "\n",
    "1) ID: <APP_ID> | Name: <APP_NAME> | Namespace: <NAMESPACE> | Cluster: <SOURCE_CLUSTER>\n",
    "2) ID: ...\n",
    "\n",
    "Please reply with the number (1 to N) or the App ID.\n",
    "\n",
    "Rules:\n",
    "- Use unique tuples only\n",
    "- Show up to 15 options\n",
    "- Use SOURCE_CLUSTER value for \"Cluster\"\n",
    "- Do not invent values\n",
    "\"\"\",\n",
    "        \"TOOLS\": [\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [\"vs_66507187-b021-4b8b-82ca-a2f277d3fcef\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"openshift\", \"server_url\": \"http://ocp-mcp-server.llama-stack.svc.cluster.local:8000/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]},\n",
    "            {\"type\": \"mcp\", \"server_label\": \"app-migration-utility\", \"server_url\": \"http://app-migration-utility-mcp.app-migration-utility.svc.cluster.local/mcp\", \"require_approval\": \"never\", \"headers\": {\"AUTHORITATIVE_USER_ID\": \"sudash@redhat.com\", \"SERVICE_NOW_TOKEN\": \"now_mock_api_key\"}, \"allowed_tools\": [\"\"]}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f093b-68ae-4955-a7ee-d3bb9d23f406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
