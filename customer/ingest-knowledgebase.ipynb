{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest `knowledge_bases/` into **pgvector** vector stores (LlamaStack)\n",
    "\n",
    "This notebook:\n",
    "- Discovers an embedding model from LlamaStack\n",
    "- Creates (or reuses) **one pgvector-backed vector store per subfolder** in `knowledge_bases/`\n",
    "- Uses **vector store name = subfolder name**\n",
    "- Ingests `.txt` as raw text\n",
    "- Ingests `.pdf` via **PyPDF (pypdf) text extraction** (offline friendly)\n",
    "\n",
    "## How to use\n",
    "- Edit the variables in the first Python cell (LlamaStack URL + knowledge bases path)\n",
    "- Run cells top-to-bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client==0.3.0 in /opt/app-root/lib64/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: pypdf in /opt/app-root/lib64/python3.12/site-packages (6.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (4.12.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (8.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (1.9.0)\n",
      "Requirement already satisfied: fire in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (0.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (3.0.52)\n",
      "Requirement already satisfied: pyaml in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (25.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (2.12.5)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (2.32.5)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (14.3.2)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.12/site-packages (from llama-stack-client==0.3.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.12/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.3.0) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.3.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/app-root/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/app-root/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.0) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/app-root/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.12/site-packages (from pandas->llama-stack-client==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.12/site-packages (from prompt-toolkit->llama-stack-client==0.3.0) (0.2.14)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.12/site-packages (from pyaml->llama-stack-client==0.3.0) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/app-root/lib64/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.12/site-packages (from requests->llama-stack-client==0.3.0) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.12/site-packages (from rich->llama-stack-client==0.3.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.12/site-packages (from rich->llama-stack-client==0.3.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client==0.3.0) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.3.0) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Optional installs (uncomment if running outside the container image)\n",
    "%pip install -U llama-stack-client==0.3.0 pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA_STACK_URL: http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\n",
      "KNOWLEDGE_BASES_DIR: /opt/app-root/src/it-self-service-agent-sukanta/knowledge_bases\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n",
    "\n",
    "# ============================\n",
    "# User inputs (edit these)\n",
    "# ============================\n",
    "LLAMA_STACK_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
    "KNOWLEDGE_BASES_DIR = Path(\"knowledge_bases\").resolve()\n",
    "\n",
    "# ============================\n",
    "# Ingestion settings\n",
    "# ============================\n",
    "VECTOR_STORE_PROVIDER_ID = \"pgvector\"  # force pgvector\n",
    "REUSE_EXISTING_VECTOR_STORES = True\n",
    "\n",
    "INGEST_TXT = True\n",
    "INGEST_PDF = True\n",
    "\n",
    "CHUNK_SIZE_TOKENS_TXT = 300\n",
    "CHUNK_SIZE_TOKENS_PDF = 512\n",
    "\n",
    "print(\"LLAMA_STACK_URL:\", LLAMA_STACK_URL)\n",
    "print(\"KNOWLEDGE_BASES_DIR:\", str(KNOWLEDGE_BASES_DIR))\n",
    "\n",
    "assert KNOWLEDGE_BASES_DIR.exists(), f\"Knowledge bases dir not found: {KNOWLEDGE_BASES_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='llama-17b/llama-4-scout-17b-16e-w4a16', metadata={}, api_model_type='llm', provider_id='llama-17b', type='model', provider_resource_id='llama-4-scout-17b-16e-w4a16', model_type='llm'), Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding'), Model(identifier='sentence-transformers/nomic-ai/nomic-embed-text-v1.5', metadata={'embedding_dimension': 768.0, 'default_configured': True}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='nomic-ai/nomic-embed-text-v1.5', model_type='embedding')]\n",
      "Embedding model: granite-embedding-125m\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "client = LlamaStackClient(base_url=LLAMA_STACK_URL)\n",
    "\n",
    "# Discover embedding model + dimension (mirrors rag-validation notebook)\n",
    "models = client.models.list()\n",
    "print(models)\n",
    "# Support both model field names depending on client version\n",
    "embedding_model = next(\n",
    "    (\n",
    "        m\n",
    "        for m in models\n",
    "        if getattr(m, \"model_type\", None) == \"embedding\"\n",
    "        or getattr(m, \"api_model_type\", None) == \"embedding\"\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "assert embedding_model is not None, \"No embedding model registered in LlamaStack\"\n",
    "\n",
    "embedding_model_id = embedding_model.identifier\n",
    "embedding_dimension = int(getattr(embedding_model, \"metadata\", {}).get(\"embedding_dimension\", 0) or 0)\n",
    "assert embedding_dimension > 0, f\"Invalid embedding dimension: {embedding_dimension}\"\n",
    "\n",
    "print(\"Embedding model:\", embedding_model_id)\n",
    "print(\"Embedding dimension:\", embedding_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/vector_stores \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncOpenAICursorPage[VectorStore](data=[], has_more=False, last_id=None, object='list', first_id=None)\n"
     ]
    }
   ],
   "source": [
    "print(client.vector_stores.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/vector_stores \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 existing vector stores\n"
     ]
    }
   ],
   "source": [
    "# Build a name -> latest vector_store_id map (used when reusing existing stores)\n",
    "stores = client.vector_stores.list()\n",
    "store_list = getattr(stores, \"data\", stores)  # some client versions return `.data`\n",
    "\n",
    "name_to_latest_store = {}\n",
    "for vs in store_list:\n",
    "    name = getattr(vs, \"name\", None)\n",
    "    if not name:\n",
    "        continue\n",
    "    created_at = getattr(vs, \"created_at\", 0) or 0\n",
    "    prev = name_to_latest_store.get(name)\n",
    "    if prev is None or created_at > prev[0]:\n",
    "        name_to_latest_store[name] = (created_at, vs.id)\n",
    "\n",
    "print(f\"Found {len(name_to_latest_store)} existing vector stores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion helpers\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def iter_kb_subfolders(root: Path) -> list[Path]:\n",
    "    # Each direct subfolder under knowledge_bases is a KB.\n",
    "    # Ignore hidden folders like `.ipynb_checkpoints`.\n",
    "    folders: list[Path] = []\n",
    "    for p in root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if p.name.startswith(\".\"):\n",
    "            continue\n",
    "        folders.append(p)\n",
    "    return sorted(folders)\n",
    "\n",
    "\n",
    "def iter_files(root: Path) -> Iterable[Path]:\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\", \".pdf\"}:\n",
    "            yield p\n",
    "\n",
    "\n",
    "def get_or_create_vector_store_id(kb_name: str) -> str:\n",
    "    # Vector store name == KB subfolder name\n",
    "    if REUSE_EXISTING_VECTOR_STORES and kb_name in name_to_latest_store:\n",
    "        vs_id = name_to_latest_store[kb_name][1]\n",
    "        print(f\"Reusing vector store for '{kb_name}': {vs_id}\")\n",
    "        return str(vs_id)\n",
    "\n",
    "    vs = client.vector_stores.create(\n",
    "        name=kb_name,\n",
    "        extra_body={\n",
    "            \"provider_id\": VECTOR_STORE_PROVIDER_ID,\n",
    "            \"embedding_model\": embedding_model_id,\n",
    "            \"embedding_dimension\": embedding_dimension,\n",
    "        },\n",
    "    )\n",
    "    print(f\"Created vector store for '{kb_name}': {vs.id}\")\n",
    "    return str(vs.id)\n",
    "\n",
    "\n",
    "def ingest_document(vector_db_id: str, doc: RAGDocument, chunk_size: int) -> None:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=[doc],\n",
    "        vector_db_id=vector_db_id,\n",
    "        chunk_size_in_tokens=chunk_size,\n",
    "    )\n",
    "\n",
    "\n",
    "def ingest_txt(vector_db_id: str, kb_root: Path, path: Path) -> None:\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    rel = path.relative_to(kb_root)\n",
    "\n",
    "    doc = RAGDocument(\n",
    "        document_id=f\"txt::{kb_root.name}/{rel.as_posix()}::{uuid.uuid4().hex[:8]}\",\n",
    "        content=text,\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={\n",
    "            \"source\": str(path),\n",
    "            \"kb_name\": kb_root.name,\n",
    "            \"relative_path\": rel.as_posix(),\n",
    "            \"file_type\": \"txt\",\n",
    "        },\n",
    "    )\n",
    "    ingest_document(vector_db_id, doc, CHUNK_SIZE_TOKENS_TXT)\n",
    "\n",
    "\n",
    "def ingest_pdf(vector_db_id: str, kb_root: Path, path: Path) -> bool:\n",
    "    \"\"\"Ingest PDF using offline-friendly text extraction via pypdf.\"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "    except ImportError:\n",
    "        print(f\"Skipping PDF (pypdf not installed): {path}\")\n",
    "        return False\n",
    "\n",
    "    rel = path.relative_to(kb_root)\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(str(path))\n",
    "        pages_text = []\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            t = page.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                pages_text.append(f\"\\n\\n--- Page {i+1} ---\\n\\n{t}\")\n",
    "\n",
    "        text = \"\".join(pages_text).strip()\n",
    "        if not text:\n",
    "            print(f\"Skipping PDF (no extractable text): {path}\")\n",
    "            return False\n",
    "\n",
    "        doc = RAGDocument(\n",
    "            document_id=f\"pdf-txt::{kb_root.name}/{rel.as_posix()}::{uuid.uuid4().hex[:8]}\",\n",
    "            content=text,\n",
    "            mime_type=\"text/plain\",\n",
    "            metadata={\n",
    "                \"source\": str(path),\n",
    "                \"kb_name\": kb_root.name,\n",
    "                \"relative_path\": rel.as_posix(),\n",
    "                \"file_type\": \"pdf\",\n",
    "                \"extraction\": \"pypdf\",\n",
    "                \"page_count\": len(reader.pages),\n",
    "            },\n",
    "        )\n",
    "        ingest_document(vector_db_id, doc, CHUNK_SIZE_TOKENS_PDF)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping PDF (pypdf failed): {path}\\n  Error: {type(e).__name__}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/vector_stores \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 KB folders\n",
      "\n",
      "=== KB: type-a-app-migration ===\n",
      "Created vector store for 'type-a-app-migration': vs_2d5a6617-574e-43dd-801b-0e3d25f65e89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Counts: {'vector_stores_used': 1, 'pdf_ingested': 1}\n"
     ]
    }
   ],
   "source": [
    "# Run ingestion (one vector store per KB subfolder)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "errors = []\n",
    "\n",
    "kb_folders = iter_kb_subfolders(KNOWLEDGE_BASES_DIR)\n",
    "print(f\"Found {len(kb_folders)} KB folders\")\n",
    "\n",
    "for kb_root in kb_folders:\n",
    "    kb_name = kb_root.name\n",
    "    print(f\"\\n=== KB: {kb_name} ===\")\n",
    "\n",
    "    vector_db_id = get_or_create_vector_store_id(kb_name)\n",
    "    counts[\"vector_stores_used\"] += 1\n",
    "\n",
    "    for path in iter_files(kb_root):\n",
    "        try:\n",
    "            if path.suffix.lower() == \".txt\" and INGEST_TXT:\n",
    "                ingest_txt(vector_db_id, kb_root, path)\n",
    "                counts[\"txt_ingested\"] += 1\n",
    "            elif path.suffix.lower() == \".pdf\" and INGEST_PDF:\n",
    "                ok = ingest_pdf(vector_db_id, kb_root, path)\n",
    "                counts[\"pdf_ingested\" if ok else \"pdf_skipped\"] += 1\n",
    "        except Exception as e:\n",
    "            errors.append((kb_name, str(path), type(e).__name__, str(e)))\n",
    "            counts[\"errors\"] += 1\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Counts:\", dict(counts))\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors (first 10):\")\n",
    "    for item in errors[:10]:\n",
    "        print(\"-\", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
