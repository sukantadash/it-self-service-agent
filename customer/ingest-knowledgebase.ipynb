{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingest `knowledge_bases/` into **pgvector** vector stores (LlamaStack)\n",
        "\n",
        "This notebook:\n",
        "- Discovers an embedding model from LlamaStack\n",
        "- Creates (or reuses) **one pgvector-backed vector store per subfolder** in `knowledge_bases/`\n",
        "- Uses **vector store name = subfolder name**\n",
        "- Ingests `.txt` as raw text\n",
        "- Ingests `.pdf` as **Markdown via Docling** (if `docling` is installed)\n",
        "\n",
        "## How to use\n",
        "- Edit the variables in the first Python cell (LlamaStack URL + knowledge bases path)\n",
        "- Run cells top-to-bottom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional installs (uncomment if running outside the container image)\n",
        "# %pip install -U llama-stack-client docling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "from llama_stack_client import LlamaStackClient, RAGDocument\n",
        "\n",
        "# ============================\n",
        "# User inputs (edit these)\n",
        "# ============================\n",
        "LLAMA_STACK_URL = \"http://llamastack-with-config-service.llama-stack.svc.cluster.local:8321\"\n",
        "KNOWLEDGE_BASES_DIR = Path(\"../agent-service/config/knowledge_bases\").resolve()\n",
        "\n",
        "# ============================\n",
        "# Ingestion settings\n",
        "# ============================\n",
        "VECTOR_STORE_PROVIDER_ID = \"pgvector\"  # force pgvector\n",
        "REUSE_EXISTING_VECTOR_STORES = True\n",
        "\n",
        "INGEST_TXT = True\n",
        "INGEST_PDF = True\n",
        "\n",
        "CHUNK_SIZE_TOKENS_TXT = 300\n",
        "CHUNK_SIZE_TOKENS_PDF = 512\n",
        "\n",
        "print(\"LLAMA_STACK_URL:\", LLAMA_STACK_URL)\n",
        "print(\"KNOWLEDGE_BASES_DIR:\", str(KNOWLEDGE_BASES_DIR))\n",
        "\n",
        "assert KNOWLEDGE_BASES_DIR.exists(), f\"Knowledge bases dir not found: {KNOWLEDGE_BASES_DIR}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = LlamaStackClient(base_url=LLAMA_STACK_URL)\n",
        "\n",
        "# Discover embedding model + dimension (mirrors rag-validation notebook)\n",
        "models = client.models.list()\n",
        "\n",
        "# Support both model field names depending on client version\n",
        "embedding_model = next(\n",
        "    (\n",
        "        m\n",
        "        for m in models\n",
        "        if getattr(m, \"model_type\", None) == \"embedding\"\n",
        "        or getattr(m, \"api_model_type\", None) == \"embedding\"\n",
        "    ),\n",
        "    None,\n",
        ")\n",
        "assert embedding_model is not None, \"No embedding model registered in LlamaStack\"\n",
        "\n",
        "embedding_model_id = embedding_model.identifier\n",
        "embedding_dimension = int(getattr(embedding_model, \"metadata\", {}).get(\"embedding_dimension\", 0) or 0)\n",
        "assert embedding_dimension > 0, f\"Invalid embedding dimension: {embedding_dimension}\"\n",
        "\n",
        "print(\"Embedding model:\", embedding_model_id)\n",
        "print(\"Embedding dimension:\", embedding_dimension)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a name -> latest vector_store_id map (used when reusing existing stores)\n",
        "stores = client.vector_stores.list()\n",
        "store_list = getattr(stores, \"data\", stores)  # some client versions return `.data`\n",
        "\n",
        "name_to_latest_store = {}\n",
        "for vs in store_list:\n",
        "    name = getattr(vs, \"name\", None)\n",
        "    if not name:\n",
        "        continue\n",
        "    created_at = getattr(vs, \"created_at\", 0) or 0\n",
        "    prev = name_to_latest_store.get(name)\n",
        "    if prev is None or created_at > prev[0]:\n",
        "        name_to_latest_store[name] = (created_at, vs.id)\n",
        "\n",
        "print(f\"Found {len(name_to_latest_store)} existing vector stores\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingestion helpers\n",
        "\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "def iter_kb_subfolders(root: Path) -> list[Path]:\n",
        "    # Each direct subfolder under knowledge_bases is a KB.\n",
        "    return sorted([p for p in root.iterdir() if p.is_dir()])\n",
        "\n",
        "\n",
        "def iter_files(root: Path) -> Iterable[Path]:\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_file() and p.suffix.lower() in {\".txt\", \".pdf\"}:\n",
        "            yield p\n",
        "\n",
        "\n",
        "def get_or_create_vector_store_id(kb_name: str) -> str:\n",
        "    # Vector store name == KB subfolder name\n",
        "    if REUSE_EXISTING_VECTOR_STORES and kb_name in name_to_latest_store:\n",
        "        vs_id = name_to_latest_store[kb_name][1]\n",
        "        print(f\"Reusing vector store for '{kb_name}': {vs_id}\")\n",
        "        return str(vs_id)\n",
        "\n",
        "    vs = client.vector_stores.create(\n",
        "        name=kb_name,\n",
        "        extra_body={\n",
        "            \"provider_id\": VECTOR_STORE_PROVIDER_ID,\n",
        "            \"embedding_model\": embedding_model_id,\n",
        "            \"embedding_dimension\": embedding_dimension,\n",
        "        },\n",
        "    )\n",
        "    print(f\"Created vector store for '{kb_name}': {vs.id}\")\n",
        "    return str(vs.id)\n",
        "\n",
        "\n",
        "def ingest_document(vector_db_id: str, doc: RAGDocument, chunk_size: int) -> None:\n",
        "    client.tool_runtime.rag_tool.insert(\n",
        "        documents=[doc],\n",
        "        vector_db_id=vector_db_id,\n",
        "        chunk_size_in_tokens=chunk_size,\n",
        "    )\n",
        "\n",
        "\n",
        "def ingest_txt(vector_db_id: str, kb_root: Path, path: Path) -> None:\n",
        "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    rel = path.relative_to(kb_root)\n",
        "\n",
        "    doc = RAGDocument(\n",
        "        document_id=f\"txt::{kb_root.name}/{rel.as_posix()}::{uuid.uuid4().hex[:8]}\",\n",
        "        content=text,\n",
        "        mime_type=\"text/plain\",\n",
        "        metadata={\n",
        "            \"source\": str(path),\n",
        "            \"kb_name\": kb_root.name,\n",
        "            \"relative_path\": rel.as_posix(),\n",
        "            \"file_type\": \"txt\",\n",
        "        },\n",
        "    )\n",
        "    ingest_document(vector_db_id, doc, CHUNK_SIZE_TOKENS_TXT)\n",
        "\n",
        "\n",
        "def ingest_pdf(vector_db_id: str, kb_root: Path, path: Path) -> bool:\n",
        "    # Returns True if ingested, False if skipped\n",
        "    try:\n",
        "        from docling.document_converter import DocumentConverter\n",
        "    except ImportError:\n",
        "        print(f\"Skipping PDF (docling not installed): {path}\")\n",
        "        return False\n",
        "\n",
        "    rel = path.relative_to(kb_root)\n",
        "\n",
        "    # Create converter per notebook run; docling can be heavy but is stable.\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(str(path))\n",
        "    markdown = result.document.export_to_markdown()\n",
        "\n",
        "    doc = RAGDocument(\n",
        "        document_id=f\"pdf-md::{kb_root.name}/{rel.as_posix()}::{uuid.uuid4().hex[:8]}\",\n",
        "        content=markdown,\n",
        "        mime_type=\"text/markdown\",\n",
        "        metadata={\n",
        "            \"source\": str(path),\n",
        "            \"kb_name\": kb_root.name,\n",
        "            \"relative_path\": rel.as_posix(),\n",
        "            \"file_type\": \"pdf\",\n",
        "            \"page_count\": len(getattr(result.document, \"pages\", []) or []),\n",
        "        },\n",
        "    )\n",
        "    ingest_document(vector_db_id, doc, CHUNK_SIZE_TOKENS_PDF)\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ingestion (one vector store per KB subfolder)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "counts = Counter()\n",
        "errors = []\n",
        "\n",
        "kb_folders = iter_kb_subfolders(KNOWLEDGE_BASES_DIR)\n",
        "print(f\"Found {len(kb_folders)} KB folders\")\n",
        "\n",
        "for kb_root in kb_folders:\n",
        "    kb_name = kb_root.name\n",
        "    print(f\"\\n=== KB: {kb_name} ===\")\n",
        "\n",
        "    vector_db_id = get_or_create_vector_store_id(kb_name)\n",
        "    counts[\"vector_stores_used\"] += 1\n",
        "\n",
        "    for path in iter_files(kb_root):\n",
        "        try:\n",
        "            if path.suffix.lower() == \".txt\" and INGEST_TXT:\n",
        "                ingest_txt(vector_db_id, kb_root, path)\n",
        "                counts[\"txt_ingested\"] += 1\n",
        "            elif path.suffix.lower() == \".pdf\" and INGEST_PDF:\n",
        "                ok = ingest_pdf(vector_db_id, kb_root, path)\n",
        "                counts[\"pdf_ingested\" if ok else \"pdf_skipped\"] += 1\n",
        "        except Exception as e:\n",
        "            errors.append((kb_name, str(path), type(e).__name__, str(e)))\n",
        "            counts[\"errors\"] += 1\n",
        "\n",
        "print(\"\\nDone.\")\n",
        "print(\"Counts:\", dict(counts))\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nErrors (first 10):\")\n",
        "    for item in errors[:10]:\n",
        "        print(\"-\", item)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
